supervise learning- learn from data labeled
1. regression -Regression predicts a number among potentially infinitely possible numbers.
2. classification -Classification predicts from among a limited set of categories (also called classes). These could be a limited set of numbers or labels such as "cat" or "dog". -  Breast cancer detection 
lets take an example and labelled it as malignant and benign,predict a number infinitely many possible outputs

unsupervised learning-data is not labelled 
1. clustering: google news ,DNA microarray,Grouping coustomer  grouping similar points together
Which is a type of unsupervised learning algorithm, takes data without labels and tries to automatically group them into clusters. And so maybe the next time you see or think of a panda, maybe you think of clustering as well. And besides clustering, there are other types of unsupervised learning as well. Let's go on to the next video, to take a look at some other types of unsupervised learning algorithms.

2. Anomaly detection - find unusual datapoint

3. dimenstionality reduction- compress data with fiewer number
_____________________________________________________________________________________________________________________________
Linear Regression - part of supervised regressive model 
how to build a model
traning set -> learning algorithm -> than functtion uhat is a predictation estimated y
f(w,b)=mx+c;
______________________________________________________________________________________________________________________________
Cost-Function- used to measure the error 
mostly used sqared error cost function
intution -cost is a measure how well our model is predicting the target price of the house. The term 'price' is used for housing data.
-------------------------
visualization libraries
import numpy as np
%matplotlib widget
import matplotlib.pyplot as plt
from lab_utils_uni import plt_intuition, plt_stationary, plt_update_onclick, soup_bowl
plt.style.use('./deeplearning.mplstyle')
------------------------------------
___________________________________________________________________________________________Gradient descent algo - what we do in this is we take baby steps to find out the minimum.
w=w-a d/dw(J(w,b))  truth assignment  a= alpha=learning rate and how large a step should be tacken 
if a is too small gradient descent may be slow 
if a is too large it will overshoot and will not reach the minimum value and may fail to converge


Gradient descent for linear regression model   
may have multiple local minimum
__________________________________________________________________________________________
__________________________________________________________________________________________
Multiple linear regression

1 Vectorization- uh can use it to find dot product 
2 gradient descent for multiple linear regreession- 
	1. Automatic convergence test
3. Feature Scalling 
	1. z score 
4.Polynomial regression
__________________________________________________________________________________________

Logestic Regression 
what logistic regression we end up doing is fit a curve that looks like this, S-shaped curve to this dataset. For this example, if a patient comes in with a tumor of this size, which I'm showing on the x-axis, then the algorithm will output 0.7 suggesting that is closer or maybe more likely to be malignant and benign. Will say more later what 0.7 actually means in this context. But the output label y is never 0.7 is only ever 0 or 1 . sigmoid fuction is used to build l0gistic regression

Sigmoid Function
If I use g of z to denote this function, then the formula of g of z is equal to 1 over 1 plus e to the negative z. Where here e is a mathematical constant that takes on a value of about 2.7, and so e to the negative z is that mathematical constant to the power of negative z.
g(z)= 1/(1+e^(-z))

Notice if z where really be, say a 100, e to the negative z is e to the negative 100 which is a tiny number. So this ends up being 1 over 1 plus a tiny little number, and so the denominator will be basically very close to 1. Which is why when z is large, g of z that is a Sigmoid function of z is going to be very close to 1. Conversely, you can also check for yourself that when z is a very large negative number, then g of z becomes 1 over a giant number, which is why g of z is very close to 0

so lets make out legistic regression model 
f(w,b) =g(w.x+b)=1/(1+e^(-(w.x+b)))
where g(w.x+b) = z


